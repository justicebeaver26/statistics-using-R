---
title: "Computational Mathematics and Statistics Project"
author: "Mannat Ahuja"
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Section 1: Dataset

### Reading the dataset

```{r}
bodyfat <- read.csv("C:/Users/Uni/Downloads/bodyfat.csv")
```

### Background of dataset

The dataset ‘BodyFat’ offers a rich foundation for studying the relationship between body fat percentage and various physical measurements, with a focus on developing predictive models. It is particularly useful for exploring multiple regression techniques, where the goal is to find a reliable, less invasive method for estimating body fat percentage compared to traditional approaches which are costlier and more time consuming. This dataset is derived from a study conducted by K.W. Penrose and colleagues at Brigham Young University, where the goal was to create a generalized body composition prediction equation using these simple body measurements. The researchers collected the data from 252 men and used the first 143 cases to develop predictive models, which were later validated using the remaining data. These models aimed to estimate body fat percentage accurately without the need for expensive or invasive methods.

### Motivation for choosing this dataset

The motivation to choose this dataset revolves around its practical and educational value in addressing a real-world health problem. Accurate body fat measurement is often expensive and inconvenient, but this dataset offers a way to develop predictive models using simple body measurements, making fat estimation more accessible. It is ideal for learning multiple regression techniques, with applications in health, fitness, and wellness industries. The dataset’s rich variables allow for advanced analysis, making it valuable for both academic and practical health tools. Additionally, it aligns with the growing demand for personalised health assessments, helping improving access to crucial health metrics in an easy and non-invasive way.

### Source of the dataset

The dataset is available on Kaggle: <https://www.kaggle.com/datasets/fedesoriano/body-fat-prediction-dataset>

### Brief description of the variables

1.  **Density:** The measured density of the body, obtained through hydrostatic weighing.
2.  **BodyFat:** Estimated body fat percentage calculated using Siri’s equation, derived from density measurements.
3.  **Age:** Age of the individual in years.
4.  **Weight:** The total weight of the individual in pounds (lbs).
5.  **Height:** Height of the individual in inches.
6.  **Neck:** The neck circumference measured in centimetres (cm).
7.  **Chest:** the chest circumference measured in centimetres (cm).
8.  **Abdomen:** The circumference of the abdomen measured in centimetres (cm).
9.  **Hip:** The circumference of the hips measured in centimetres (cm).
10. **Thigh:** The circumference of the thigh measured in centimetres (cm).
11. **Knee:** The circumference of the knee measured in centimetres (cm).
12. **Ankle:** The circumference of the of the ankle measured in centimetres (cm).
13. **Biceps:** The circumference of the biceps measured in centimetres (cm).
14. **Forearm:** The circumference of the forearm measured in centimetres (cm).
15. **Wrist:** The circumference of the wrist measured in centimetres (cm).

## Section 2: Distribution and Estimation

### Histogram of distribution

The chosen variable for estimation is Weight. Based on the histogram. We can tell that the distribution is positively skewed with a long tail on the right.

```{r}
# Create a histogram
hist(bodyfat$Weight, 
     breaks = 50,
     main = "Histogram of Weight",  # Title of the histogram
     xlab = "Weight (lbs)",          # Label for the X-axis
     ylab = "Frequency",              # Label for the Y-axis
     col = "skyblue",                 # Color for the bars
     border = "black",
     freq = FALSE)                    # Use density instead of frequency

# Density curve
lines(density(bodyfat$Weight), 
      col = "red",                   # Color for the density curve
      lwd = 2)                       # Line width for the density curve
```

## Suggested parametric distributions

#### Log-Normal Distribution

The log-normal distribution is a statistical distribution of a random variable whose logarithm is normally distributed. This means if Y is a random variable that is normally distributed, then $X = e^Y$ is log-normally distributed. Log-normal distribution is commonly used to model data that is positively skewed.

The log-normal distribution is characterised by two parameters: μ, which is the mean of the natural logarithm of the variable, and σ, the standard deviation of the natural logarithm of the variable.

The probability density function (PDF) is given by:

$$
f(x; \mu, \sigma) = \frac{1}{x \sigma \sqrt{2 \pi}} e^{-\frac{(\ln x - \mu)^2}{2 \sigma^2}}, \quad x > 0
$$

Mean of log-normal distribution is $e^{ \mu + \frac{\sigma^2}{2}}$ and the variance is given by $\left(e^{\sigma^2} - 1\right) e^{2\mu + \sigma^2}$ .

This distribution is commonly used for modelling data that are positively skewed, where the data cannot be negative, and where the variable might have multiplicative effects, Since Weight cannot be negative and its histogram is positively skewed, this distribution is suitable.

#### Gamma Distribution

Gamma Distribution is a statistical distribution that is defined for positive values. The Gamma distribution has two parameters: Shape parameter $\alpha$ which controls the shape of the distribution, and rate parameter $\beta$ which controls the scale of t

The probability density function (PDF) is given by:

$$
f(x; \alpha, \beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\beta x}, \quad x > 0
$$

$\Gamma(\alpha)$ is the gamma function.

The mean of gamma distribution is $k\theta$ and the variance is given by $k \theta^2$

The Gamma distribution is suitable for modelling right-skewed continuous data and is flexible and can be used to model data of different shapes. Given the shape of Weight in the histogram, this distribution is also suitable.

We plotted Q-Q plots to check the fit of both the distributions of the data. Since both the distributions lie on the 45-degree line (except for a few outliers), we can say that both the distributions fit well with the data.

### QQ Plots

```{r}
# Load necessary library
library(MASS)

# 1. Fit Log-Normal Distribution
lognormal_fit <- fitdistr(bodyfat$Weight, "lognormal")

# 2. Fit Gamma Distribution
gamma_fit <- fitdistr(bodyfat$Weight, "gamma")

# Q-Q Plot for Log-Normal Distribution
qqplot(
  qlnorm(ppoints(length(bodyfat$Weight)), meanlog = lognormal_fit$estimate[1], sdlog = lognormal_fit$estimate[2]),
  bodyfat$Weight, 
  main = "Q-Q Plot for Log-Normal Distribution",
  xlab = "Theoretical Quantiles (Log-Normal)", 
  ylab = "Sample Quantiles",
  col = "blue",
  pch = 19
)
abline(0, 1, col = "red", lwd = 2)  # 45-degree line for reference

# Q-Q Plot for Gamma Distribution
qqplot(
  qgamma(ppoints(length(bodyfat$Weight)), shape = gamma_fit$estimate[1], rate = gamma_fit$estimate[2]),
  bodyfat$Weight, 
  main = "Q-Q Plot for Gamma Distribution",
  xlab = "Theoretical Quantiles (Gamma)", 
  ylab = "Sample Quantiles",
  col = "blue",
  pch = 19
)
abline(0, 1, col = "red", lwd = 2)  # 45-degree line for reference
```

## Methods of Moments Estimations

### Sample mean and sample variance

We will first calculate the sample mean ($\bar{x}$) and the sample variance ($s^2$).

```{r}
weight <- bodyfat$Weight
mean_data <- mean(weight)
var_data <- var(weight)

cat("Sample mean =", mean_data, "\n")
cat("Sample variance =", var_data, "\n")
```

### Log-Normal Distribution

Let $X_1, X_2, \dots, X_n$ be the weights distributed log-normally. The method of moments estimators will be denoted by $\hat{\mu}$ and $\hat{\sigma}^2$.

We know that $E(X) = e^{\mu + \frac{\sigma^2}{2}}$. Therefore, the first moment is

$m_1 = \bar{x} = e^{\mu + \frac{\sigma^2}{2}},$ where $\bar{x}$ is the sample mean.

We also know that $E(X^2) = e^{2\mu + 2\sigma^2}$. Therefore, the second moment is

$m_2 = s^2 = e^{2\mu + 2\sigma^2},$ where $s^2$ is the sample variance.

After solving both equations for $\mu$ and $\sigma^2$, we get the method of moments estimators for the log-normal distribution by the following equations:

$\hat{\sigma}^2 = \log\left(1 + \frac{s^2}{\bar{x}^2}\right)$

$\hat{\mu} = \log\left(\frac{\bar{x}}{\sqrt{s^2 + \bar{x}^2}}\right)$

where $\bar{x}$ is the sample mean and $s^2$ is the sample variance.

According to our calculations, we have:

$\hat{\mu} = 5.173652, \quad \hat{\sigma} = 0.1631626 \quad (\hat{\sigma}^2 = 0.02662205).$

```{r}
# Estimating log-normal parameters using method of moments
sigma_sq_mom <- log(1 + (var_data / mean_data^2))
mu_mom <- log(mean_data^2/(sqrt(var_data + mean_data^2)))

cat("Method of Moments Estimators for Log-Normal Distribution:\n")
cat("Meanlog (µ):", mu_mom, "\n")
cat("Varlog (σ^2):", sigma_sq_mom, "\n")
cat("Sdlog (σ):", sqrt(sigma_sq_mom), "\n\n")
```

### Gamma Distribution

Let $X_1, X_2, \dots, X_n$ be the weights having a gamma distribution. The method of moments estimators will be denoted by $\hat{\alpha}$ and $\hat{\beta}$.

We know that $E(X) = \frac{\alpha}{\beta}.$

Therefore, the first moment is $m_1 = \bar{x} = \frac{\alpha}{\beta},$ where $\bar{x}$ is the sample mean.

We also know that $E(X^2) = \frac{\alpha}{\beta^2}.$

Therefore, the second moment is $m_2 = s^2 = \frac{\alpha}{\beta^2},$ where $s^2$ is the sample variance.

After solving both equations for $\alpha$ and $\beta$, we get the method of moments estimators for the gamma distribution by the following equations:

$\hat{\alpha} = \frac{\bar{x}^2}{s^2}$

$\hat{\beta} = \frac{\bar{x}}{s^2}.$

According to our calculations, we have:

$\hat{\alpha} = 37.06507, \quad \hat{\beta} = 0.2071549.$

```{r}
# Estimating gamma parameters using method of moments
shape_mom <- mean_data^2 / var_data
rate_mom <- mean_data / var_data

cat("Method of Moments Estimators for Gamma Distribution:\n")
cat("Shape (α):", shape_mom, "\n")
cat("Rate (β):", rate_mom, "\n")
```

### Plotting the distributions with Method of Moments estimators

Both the distributions with the estimated method of moments parameters almost overlap each other.

```{r}

# Parameters for log-normal distribution
meanlog <- 5.173652
sdlog <- 0.1631626

# Parameters for gamma distribution
alpha <- 37.06507
beta <- 0.2071549

# Create a sequence of x values
x <- seq(0, 300, length.out = 1000)

# Log-normal distribution
lognormal_pdf <- dlnorm(x, meanlog = meanlog, sdlog = sdlog)

# Gamma distribution
gamma_pdf <- dgamma(x, shape = alpha, rate = beta)

# Plot the distributions
plot(x, lognormal_pdf, type = "l", col = "blue", lwd = 2, 
     ylab = "Density", xlab = "x", main = "Log-normal and Gamma Distribution Comparison (MoM)")
lines(x, gamma_pdf, col = "red", lwd = 2)

# Add smaller legend below the plot
legend("bottom", legend = c("Log-normal Distribution", "Gamma Distribution"),
       col = c("blue", "red"), lwd = 2, cex = 0.8, inset = -0.2, xpd = TRUE)
```

## Maximum Likelihood Estimations

### Log-Normal DIstribution

Let $x_1, x_2, \dots, x_n$ be drawn from the lognormal distribution.

The likelihood function for $n$ observations is given by:

$L(\mu, \sigma \mid x_i) = \prod_{i=1}^{n} \frac{1}{x_i \sigma \sqrt{2\pi}} \exp\left(-\frac{(\ln x_i - \mu)^2}{2\sigma^2}\right)$

The log-likelihood function is given by:

$l(\mu, \sigma \mid x_i) = -n \log(\sigma \sqrt{2\pi}) - \sum_{i=1}^{n} \log(x_i) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (\log(x_i) - \mu)^2$

According to our calculations, we have:

$\hat{\mu} = 5.1743285, \quad \hat{\sigma} = 0.1574791 \quad (\hat{\sigma}^2 = 0.0247997).$

```{r}
data <- bodyfat$Weight

# Log-likelihood function for the Log-Normal distribution
log_likelihood_lognormal <- function(params, data) {
  # Extract parameters
  mu <- params[1]     # Mean of log(x)
  sigma <- params[2]  # Standard deviation of log(x)
  
  # Ensure the standard deviation (sigma) is positive
  if (sigma <= 0) {
    return(-Inf)  # Return -Inf if sigma is invalid
  }
  
  # Calculate log-likelihood
  n <- length(data)  # Sample size
  log_data <- log(data)  # Log of the data
  
  # Log-likelihood expression
  log_lik <- -n * log(sigma * sqrt(2 * pi)) - sum(log(data)) - 
             (1 / (2 * sigma^2)) * sum((log_data - mu)^2)
  
  return(log_lik)
}

# Defining the negative log-likelihood function for optimization
neg_log_likelihood <- function(params, data) {
  -log_likelihood_lognormal(params, data)  # We want to maximize log-likelihood, so minimize negative
}

# Providing an initial guess for mu and sigma
initial_params <- c(1, 1)  # Initial guess for mu and sigma

# Optimizing the log-likelihood function
result <- optim(par = initial_params, fn = neg_log_likelihood, data = data, method = "L-BFGS-B", lower = c(-Inf, 0.001))

# Print the estimated parameters (mu and sigma)
cat("Maximum Likelihood Estimators for Log-Normal Distribution:\n")
print(result$par)
```

### Gamma Distribution

Let $x_1, x_2, \dots, x_n$ be drawn from the Gamma distribution.

The likelihood function of the Gamma distribution is given by:

$L(\alpha, \beta \mid x_i) = \prod_{i=1}^{n} \frac{\beta^\alpha}{\Gamma(\alpha)} x_i^{\alpha-1} \exp(-\beta x_i)$

The log-likelihood function is given by:

$l(\alpha, \beta \mid x_i) = (\alpha - 1) \sum_{i=1}^{n} \log(x_i) - n \log(\Gamma(\alpha)) - n \alpha \log(\beta) - \beta \sum_{i=1}^{n} x_i$

According to our calculations, we have:

$\hat{\alpha} = 39.7409449, \quad \hat{\beta} = 0.2221093.$

```{r}
data <- bodyfat$Weight  

# Log-likelihood function for the Gamma distribution
log_likelihood_gamma <- function(params, data) {
  # Extracting parameters
  alpha <- params[1]  # Shape parameter
  beta <- params[2]   # Rate parameter (inverse of scale)
  
  if (alpha <= 0 | beta <= 0) {
    return(-Inf)  # Return -Inf if parameters are invalid
  }
  
  # Calculate log-likelihood
  n <- length(data)  # Sample size
  log_lik <- n * (alpha * log(beta) - lgamma(alpha)) +
             (alpha - 1) * sum(log(data)) - beta * sum(data)  
  
  return(-log_lik)  # Return negative log-likelihood for optimization
}

# Performing optimization to find MLEs for alpha and beta
# Providing an initial guess for alpha and beta
initial_params <- c(1, 1)  # Initial guess for shape and rate parameters

# Optimizing the log-likelihood function
result <- optim(par = initial_params, log_likelihood_gamma, data = data, method = "L-BFGS-B", lower = c(0.001, 0.001))

# Print the estimated parameters (alpha and beta)
cat("Maximum Likelihood Estimators for Gamma Distribution:\n")
print(result$par)
```

### Plotting the Distributions for Maximum Likelihood Estimates

Both the distributions with the estimated MLE parameters almost overlap each other.

```{r}

# Parameters for log-normal distribution
meanlog <- 5.1743285 
sdlog <- 0.1574791

# Parameters for gamma distribution
alpha <- 39.7409449
beta <- 0.2221093

# Create a sequence of x values
x <- seq(0, 300, length.out = 1000)

# Log-normal distribution
lognormal_pdf <- dlnorm(x, meanlog = meanlog, sdlog = sdlog)

# Gamma distribution
gamma_pdf <- dgamma(x, shape = alpha, rate = beta)

# Plot the distributions
plot(x, lognormal_pdf, type = "l", col = "blue", lwd = 2, 
     ylab = "Density", xlab = "x", main = "Log-normal and Gamma Distribution Comparison (MLE)")
lines(x, gamma_pdf, col = "red", lwd = 2)

# Add smaller legend below the plot
legend("bottom", legend = c("Log-normal Distribution", "Gamma Distribution"),
       col = c("blue", "red"), lwd = 2, cex = 0.8, inset = -0.1, xpd = TRUE)
```

## Section 3: Linear Regression

### Rationale for fitting the model

We will build a linear regression model using BodyFat as the response variable and Density, Knee, Height, and Wrist as the independent variables.

Fitting a linear regression model with BodyFat as the response variable and Density, Knee, Height, and Wrist as the independent variables can serve several purposes and provide valuable insights. The model's primary goal is to predict body fat percentage based on easily measurable physical characteristics. These variables are commonly available through routine physical measurements, making it easier to predict body fat percentage. Fitting the linear model will help us quantify the relationship between the response variable and the predictors. Body density is inversely related to body fat percentage, which means it should be negatively correlated with body fat. Circumference of knee is usually positively correlated with body fat. The relationship between Height and body may not be as strong, but it may influence body fat. Wrist circumference is often a strong indicator of a higher body fat, which makes them positively correlated. Fitting the linear model will, therefore, help us understand these relationships. It will help us determine which variable is the most influential in predicting body fat percentage. The correlation plot below confirms these relationships between the response variable and the predictors. The choice of independent variables is to avoid multicollinearity (discussed later).

Body fat percentage is a key indicator of health, fitness, and disease risk (for instance, cardiovascular diseases and diabetes). By understanding how these predictor variables relate to body fat, we can develop models for screening obesity-related health risks and assess fitness levels based on easily measurable factors.

```{r}
bodyfat_variables <- bodyfat[, c("BodyFat", "Density", "Knee", "Height", "Wrist")]
pairs(bodyfat_variables, main = "Correlation Plot")
```

### Justification for the Response Variable

‘BodyFat’ is our chosen response variable. The choice of BodyFat as the response variable in the linear regression model is justified for several reasons.

Body fat is a crucial metric in assessing overall health, fitness, and risk for various diseases. High levels of body fat are associated with increased risks of conditions such as cardiovascular disease, diabetes, hypertension, while extremely low levels could indicate malnutrition or eating disorders. Thus, predicting body fat based on physical measurements can provide important insights. It also makes it easier to measure by using variables that can be quantified, since measuring body fat directly is an expensive process. Using body fat as a response variable to develop a model can be a cost-effective and a convenient alternative.

Body fat also has a strong relationship with the independent variables, as stated above. By using it as the response variable, the model can harness the predictive power of these easily measurable characteristics. This will also help us gather insights regarding the practical implications of using BodyFat as the response variable. The model can be used for screening obesity and related issues, to monitor changes in body composition over time and assess the effectiveness of training programs, and to guide dietary adjustments aimed at healthy weight loss or gain.

The choice of BodyFat as the response variable is justified because it is a key health indicator, difficult to measure directly, and has strong relationships with independent variables. It offers us practical value in healthcare, fitness, and nutrition and allows us to make detailed and meaningful predictions based on physical characteristics.

### Fitting a Linear Regression Model

#### Intercept term

The intercept term is estimated to be 466.32. This is the predicted value of BodyFat when all predictor variables (Density, Knee, Height, and Wrist) are equal to zero. In practice, this value does not serve much purpose on its own since it is unrealistic to assume the values for some of these variables as zero, but it serves as a baseline model.

#### The Most Influential Variable

Density is the most influential predictor of BodyFat. The estimate for Density is -427.994. This means that by keeping other factors constant, a 1 unit increase in the Density reduces the BodyFat by about 427%, indicating a strong negative relationship. The P-value is less than 0.05, which means that Density as a predictor of BodyFat is statistically significant. The change in R-squared after removing density is about 0.2554, which means model's explanatory power falls by 25.62% if we remove Density.

```{r}
# Fit the linear model
lm_model <- lm(BodyFat ~ Density + Knee + Height + Wrist, data = bodyfat)

# Summary of the model
summary(lm_model)
```

```{r}
# Computing the R-squared for the full model
r2_all <- summary(lm_model)$r.squared

# Listing of predictors to be removed one at a time
predictors <- c("Density", "Weight", "Height", "Abdomen")

# Creating an empty vector to store the change in R-squared
change_r2 <- c()

# Looping through predictors and fit models without each predictor
for (i in predictors) {
  # Fit the model without one predictor
  tmp_formula <- as.formula(paste("BodyFat ~", paste(predictors[predictors != i], collapse = " + ")))
  fit_1 <- lm(tmp_formula, data = bodyfat)
  
  # Computing the change in R-squared
  r2_diff <- r2_all - summary(fit_1)$r.squared
  change_r2 <- c(change_r2, r2_diff)
  
  # Printing the change in R-squared
  cat("Change in R² after removing", i, ":", r2_diff, "\n")
}
```

### Fit Diagnostics

The R-squared for the model is 0.9765, which means that the independent variables explain 97.65% of the variability in the response variable, which is very good. The P-value for Density is less than 0.05, making it a significant predictor of BodyFat. However, the P-values for Knee, Height, and Wrist are more than 0.05, indicating that we do not have much evidence to suggest whether they influence Bodyfat or not. However, the overall P-value is less than 0.05, which makes the model a good fit.

#### Diagnostic Plots

**Residual vs Fitted Plot:** This plot helps check for non-linearity, heteroscedasticity and outliers. The plot suggests that the residuals are mostly randomly scattered around the horizontal line. However, there appear to be some outliers that need to be considered. There is no strong evidence of heteroscedasticity, and the residuals spread fairly well.

**Q-Q Plot:** This plot checks whether the residuals are normally distributed. Most residuals are normally distributed, except for a few outliers.

**Scale Location:** This plot checks homoscedasticity. The red smooth line is relatively flat and does not show strong trend, suggesting that the variance of the residuals is fairly constant. But a few outliers are present.

**Residuals vs Leverage:** This plot helps identify influential observations that may affect the model fit. Most points have low leverage, indicating they are not overly influential in determining the regression line, but the outliers may strongly influence the model.

The overall fit of the model is fairly good, but there are a few outliers that need to be considered.

```{r}
# Diagnostic plots
par(mfrow = c(2, 2))  # Set up a 2x2 plotting area
plot(lm_model)        # Create diagnostic plots
```

Multicollinearity occurs when two or more predictors are highly correlated, making it difficult to interpret the results of a regression analysis. A high Variance Inflation Factor indicates multicollinearity. Since the VIF for all the independent variables is less than 5, we can say there is no multicollinearity in the model.

```{r}
# Check for multicollinearity
library(car)
vif_values <- vif(lm_model)
print(vif_values)
```

## Section 4: Principal Component Analysis

### Suitability of Performing PCA on the Variables

Several factors justify the suitability of performing a Principal Component Analysis (PCA) on the given variables, where BodyFat is the target variable.

**1. Multicollinearity:** Most predictor variables such as Weight, Height, Abdomen, etc are likely to be correlated. Body dimensions are usually highly correlated, which may lead to multicollinearity. PCA helps overcome this problem by transforming original variables into uncorrelated principal components.

**2. Reduction in Dimensionality:** The data includes 15 variables. The model may be prone to overfitting due to many variables. PCA helps overcome this problem by transforming the dataset into a smaller number of principal components and maintaining the critical information.

**3. Interpretability:** Using many correlated variables can make it challenging to interpret model results. By reducing the dataset to fewer components that represent major variation patterns among the variables, the model becomes easier to understand.

**4. Data Structure:** PCA is generally suitable for continuous variables that are on similar scales. PCA is a suitable technique since all numeric variables can be standardised if necessary.

### Rationale of Performing PCA on the Variables

A dataset with 15 variables could have high dimensionality and be difficult to visualise. PCA transforms the data and reduces the number of variables into smaller sets of uncorrelated components while retaining most of the variance. This helps simplify further analyses, primarily when visualising individual relationships or using machine learning algorithms. By doing this, we can eliminate the irrelevant variables which may be highly correlated. Hence, it enables us to focus on the most informative aspects of the data, by filtering out the ‘noise’ and discarding the irrelevant variables. For example, variables like chest, wrist, abdomen, etc. are often positively correlated which may lead to multicollinearity. This may result in skewed results, making it difficult to identify the independent impact of individual predictors. PCA therefore addresses multicollinearity by transforming the variables.

PCA can help us understand the underlying patterns in the data that may otherwise not be obvious. For example, the principal components may reveal latent variables such as overall body size captured by the combination of weight, height, and the measurements, or the body fat distribution captured by abdomen, hip, and chest measurements. These components provide more interpretable results of how various measurements relate to one another and the body’s structure. Principal components can be used as input features instead of the original measurements. They often provide better predictive power by removing multicollinearity and capturing the most significant variation. The size of the dataset, the dimensionality, and all the advantages of using PCA justify its use.

### Reason for Standardization

The summary of the variables reveals that they have different scales and dimensionalities, particularly Density and other measurement variables like Weight, Chest, Abdomen, etc. Therefore, it will be better to standardize the data.

```{r}
summary(bodyfat)
```

### Number of Principal Components Retained

The Importance of Components table summarises the variance explained by each principal component (PC). There are 15 variables, but only 14 PCs formed because PCA reduces the dimensionality of the data, and the 15^th^ PC would not add anything new to the analysis. Component 1 has the largest standard deviation and explains about 60.27% of the variance. Component 2 explains an additional variance of about 11.23%, bringing the cumulative variance by the first two components to 71.50%. Component 3 explains about 7.49% of the variance. Together, the first three components explain about 78.99% of variance.

#### Standardising because of different scales

```{r}
pc = princomp(bodyfat[,-2],cor=T)
# -2 because the BodyFat column is the target variable
summary(pc)
```

#### Scree Plot

Scree plot is used to visualise the variances (or eigenvalues) associated with each principal component. The x-axis represents the principal components, and the y-axis represents the variance explained by each principal component.

It shows that component 1 explains most of the variance (about 8) and captures a significant portion of the total variability in the data. Each subsequent component explains less variance, There is a dramatic drop after Component 1, and after Component 3, the variance levels off, indicating that additional components contribute very little to the variance.

The plot shows an “elbow” point at Component 2 or 3, suggesting that the remaining components add minimal value after these components. This suggests that we may only need to retain the first 2 or 3 components for dimensionality reduction, as they capture most of the variability.

```{r}
plot(pc,type="l") #scree plot
```

### Interpretation of the First Two Principal Components

To interpret the PCs, we will need to check the coefficients, or “loadings”.

Loadings represent each original variable's weight or contribution to the corresponding principal component. Higher absolute values indicate that the variable contributes more to the principal component.

To interpret the first PC, we will check which coefficients are “large”, relative to others. Most body measurements (for example, weight, neck, chest, abdomen, hip, thigh) have negative loadings, suggesting that Component 1 might represent a general measure of body size or volume. Density has a positive loading on Component 1 (0.226), which indicates that individuals with a higher body density (lower body fat) tend to have higher values along this component.

For the second PC, we see that Age has the largest negative loading (-0.595) , indicating that this component might be related to Age since it is the primary driver of this component. Height and Wrist have positive loadings, suggesting that as Component 2 increases, height and wrist size tend to increase.

#### Checking loadings

```{r}
pc$loadings
```

#### Scatter Plot

The PCA biplot is a scatter plot of the first two PCs that helps us visualize how the data is spread across these components. From the scatter plot, we conclude that the first two PCs do not have a very high discriminatory power, and we cannot tell with a high accuracy what class the target variable belongs to.

```{r}
plot(pc$scores[,1],pc$scores[,2],col=bodyfat_variables[,1],
xlab="PC1",ylab="PC2")
```
